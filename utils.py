import requests
from functools import lru_cache
from config import LLAMA_CHAT_API_URL, LLAMA_GENERATE_API_URL, MODEL_NAME, CACHE_MAX_SIZE
import traceback

from postgre_db import get_categories

@lru_cache(maxsize=CACHE_MAX_SIZE)
def cached_llama_call(prompt: str) -> dict:
    payload = {"model": MODEL_NAME, "prompt": prompt, "stream": False}
    response = requests.post(LLAMA_GENERATE_API_URL, json=payload)
    return response.json()

async def ocr_process(image_path: str) -> tuple[str, dict]:
    text = f"Chi 1000000 cho th·ª±c ph·∫©m t·∫°i VinMart, 2025-04-15"
    metadata = {"store": "VinMart", "location": "H√† N·ªôi"}
    return text, metadata

async def analyze_sentiment_advanced(text: str) -> tuple[str, float]:
    prompt = f"Ph√¢n t√≠ch c·∫£m x√∫c c·ªßa vƒÉn b·∫£n sau: '{text}'"
    response = cached_llama_call(prompt)
    sentiment = "lo l·∫Øng" if "lo l·∫Øng" in text else "b√¨nh th∆∞·ªùng"
    score = 0.9 if "lo l·∫Øng" in text else 0.5
    return sentiment, score

async def extract_receipt_info_advanced(text: str) -> tuple[dict, dict]:
    parts = text.split(",")
    amount = float(parts[0].split()[-2])
    category = parts[0].split("cho")[-1].split("t·∫°i")[0].strip()
    date = parts[1].strip()
    metadata = {"store": parts[0].split("t·∫°i")[-1].strip(), "timestamp": "2025-04-15T10:00:00"}
    return {"date": date, "amount": amount, "category": category, "source": "bi√™n lai"}, metadata

async def predict_spending(transactions: list[dict]) -> list[dict]:
    total = sum(t["amount"] for t in transactions)
    return [{"date": "2025-05-01", "predicted_amount": total * 1.1, "category": "th·ª±c ph·∫©m", "confidence": 0.85}]


import re
import datetime
import httpx

async def extract_receipt_info_advanced(text: str):
    match = re.search(r'(\d+(?:[\.,]?\d+)?)(k|K|ngh√¨n|tr|tri·ªáu)?', text)
    if not match:
        raise ValueError("Kh√¥ng t√¨m th·∫•y s·ªë ti·ªÅn trong n·ªôi dung.")

    num = match.group(1).replace(",", ".")
    unit = match.group(2) or ""
    amount = float(num)

    if unit.lower() in ['k', 'ngh√¨n']:
        amount *= 1_000
    elif unit.lower() in ['tr', 'tri·ªáu']:
        amount *= 1_000_000

    transaction = {
        "amount": amount,
        "note": text,
    }
    metadata = {
        "source": "receipt",
        "raw_text": text
    }
    return transaction, metadata

# =============================
# 2. Rule-based fallback
# =============================
def classify_category(text: str) -> str:
    text = text.lower()

    categories = {
        "th·ª±c ph·∫©m": ["g·∫°o", "rau", "th·ªãt", "tr·ª©ng", "s·ªØa", "ƒÉn", "si√™u th·ªã", "vinmart"],
        "ti·ªÅn nh√†": ["thu√™ nh√†", "ti·ªÅn nh√†", "ph√≤ng tr·ªç", "nh√† nguy√™n cƒÉn"],
        "ti·ªÅn ƒëi·ªán": ["ti·ªÅn ƒëi·ªán", "h√≥a ƒë∆°n ƒëi·ªán", "evn"],
        "ti·ªÅn n∆∞·ªõc": ["ti·ªÅn n∆∞·ªõc", "h√≥a ƒë∆°n n∆∞·ªõc", "n∆∞·ªõc sinh ho·∫°t"],
        "ƒëi·ªán tho·∫°i / internet": ["viettel", "vina", "mobifone", "wifi", "data", "4g", "5g", "internet"],
        "ƒëi l·∫°i": ["grab", "taxi", "xƒÉng", "xe", "bus", "toll"],
        "gi·∫£i tr√≠": ["netflix", "karaoke", "xem phim", "r·∫°p", "game", "spotify"],
        "mua s·∫Øm": ["qu·∫ßn √°o", "gi√†y", "m·ªπ ph·∫©m", "shopee", "lazada", "tiki", "ph·ª• ki·ªán"],
        "gi√°o d·ª•c": ["h·ªçc ph√≠", "kh√≥a h·ªçc", "ti·∫øng anh", "s√°ch", "l·ªõp h·ªçc"],
        "y t·∫ø": ["thu·ªëc", "kh√°m", "b·ªánh vi·ªán", "hi·ªáu thu·ªëc", "y t·∫ø", "b·∫£o hi·ªÉm y t·∫ø"],
    }

    for category, keywords in categories.items():
        if any(kw in text for kw in keywords):
            return category

    return "kh√°c"

# =============================
# 3. LLM classification (Ollama /api/chat)
# =============================

GOOGLE_GEMINI_URL="https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent"

async def classify_category_llm(text: str) -> tuple[str, str]:
    categories = await get_categories()
    categories_str = "\n".join([f"{cat.id}:{cat.name}" for cat in categories])
    print("CATEGORIES:")
    print(categories_str)
    print("TEXT:")
    prompt = f'T√¥i c√≥ m·ªôt danh s√°ch c√°c danh m·ª•c:\n{categories_str}.\nM·ªói danh m·ª•c c√≥ 1 uuid v√† t√™n, c√°ch nhau b·ªüi d·∫•u \":\".\nT√¥i s·∫Ω cho b·∫°n 1 c√¢u n√≥i, h√£y ph√¢n lo·∫°i n·ªôi dung c√¢u n√≥i ƒë√≥ v√†o m·ªôt trong c√°c nh√≥m danh m·ª•c tr√™n.\nCh·ªâ tr·∫£ l·ªùi ƒë√∫ng danh m·ª•c duy nh·∫•t theo c√∫ ph√°p "id:t√™n_danh_m·ª•c".\n\nC√¢u: \"{text}\"'
    print(prompt)
    try:
        async with httpx.AsyncClient(timeout=20.0) as client:
            res = await client.post(
                GOOGLE_GEMINI_URL,
                json={
                    "contents": [
                        {
                            "parts": [
                                {
                                    "text": prompt
                                }
                            ]
                        }
                    ]
                },
                params={
                    "key":"AIzaSyCCsuoRfyhdeMLKyuzi4ae-aUsCKT5ivoQ"
                }
            )
            res.raise_for_status()
            data = res.json()
            text = data["candidates"][0]["content"]["parts"][0]["text"].strip().lower()
            return text
    except Exception:
        print("‚ö†Ô∏è L·ªói khi ph√¢n lo·∫°i b·∫±ng LLM:")
        traceback.print_exc()
        
        return "748a7d51-c8e4-48b6-8e8f-eb59bc341978:kh√°c"

# =============================
# 4. T·∫°o ph·∫£n h·ªìi h√†i h∆∞·ªõc (LLM via /api/chat)
# =============================
import httpx
import traceback

import httpx
import traceback

async def generate_funny_response(transaction: dict) -> str:
    prompt = f"""
Ng∆∞·ªùi d√πng v·ª´a chi {int(transaction['amount']):,} VND cho {transaction['category']} v·ªõi ghi ch√∫: "{transaction['note']}".
Vi·∫øt m·ªôt c√¢u ph·∫£n h·ªìi h√†i h∆∞·ªõc, ch√¢m bi·∫øm, ch·ªâ ch·ª≠i ng∆∞·ªùi d√πng n·∫øu chi ti√™u kh√¥ng h·ª£p l√Ω, t·ªëi ƒëa 2 c√¢u. Nh∆∞ng ch·ªâ c·∫ßn 1 c√¢u l√† ƒë·ªß.
"""

    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            res = await client.post(
                LLAMA_CHAT_API_URL,
                json={
                    "model": "llama3.2",
                    "messages": [{"role": "user", "content": prompt}],
                    "stream": False
                }
            )
            res.raise_for_status()
            data = res.json()
            response_text = data.get("message", {}).get("content", "").strip()
            if not response_text:
                return "ü§ñ Bot b√≠ qu√°, ch∆∞a nghƒ© ra c√¢u n√†o h√†i!"
            return response_text

    except Exception:
        print("‚ö†Ô∏è L·ªói khi t·∫°o c√¢u h√†i h∆∞·ªõc:")
        traceback.print_exc()
        return "ü§ñ Bot ƒëang l·ªói k·ªπ thu·∫≠t, kh√¥ng th·ªÉ pha tr√≤ l√∫c n√†y."



# =============================
# 5. Dummy support
# =============================

async def ocr_process(image_path: str):
    return "200k ti·ªÅn g·∫°o ·ªü B√°ch H√≥a Xanh", {"ocr_engine": "dummy", "image_path": image_path}



async def predict_spending(transactions: list):
    if not transactions:
        return []
    return [{
        "category": transactions[0]["category"],
        "predicted_amount": transactions[0]["amount"] * 1.2,
        "confidence": 0.87
    }]
    
